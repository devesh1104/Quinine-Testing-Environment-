# âš¡ Quick Reference - Local GGUF Setup

## ğŸ¯ What Was Set Up For You

| Component | Details | Status |
|-----------|---------|--------|
| **Model** | Mistral 7B (Q4_K_M quantization) | âœ… Ready |
| **Adapter** | `local_gguf_adapter.py` | âœ… Created |
| **Configuration** | `config/config_local.yaml` | âœ… Created |
| **Quickstart** | `quickstart_local.py` | âœ… Created |
| **Documentation** | `LOCAL_GGUF_SETUP.md` | âœ… Created |
| **Project Structure** | `PROJECT_STRUCTURE.md` | âœ… Organized |

---

## ğŸš€ 3-Step Quick Start

### 1. Install Dependency
```powershell
pip install llama-cpp-python
```

### 2. Run Tests
```powershell
cd "C:\Users\acer\Desktop\Intership@quinine\Official Work-week 4\llm-security-testing-framework"
python quickstart_local.py
```

### 3. View Results
```powershell
# Reports in ./reports/report_*.html
start (Get-ChildItem "./reports/report_*.html" | Sort CreationTime -Descending | Select -First 1)
```

---

## ğŸ“ Key Files Created/Modified

### New Files Created âœ¨
- `src/adapters/local_gguf_adapter.py` - GGUF model handler
- `config/config_local.yaml` - Configuration for your setup
- `quickstart_local.py` - Main entry point
- `LOCAL_GGUF_SETUP.md` - This setup guide
- `PROJECT_STRUCTURE.md` - Project organization
- `requirements-local.txt` - Dependencies

### Updated/Enhanced Files
- `src/adapters/base.py` - Added LOCAL_GGUF model type
- `src/orchestrator.py` - Registered GGUF adapter

### Model File Location ğŸ“
```
C:\Users\acer\Desktop\Intership@quinine\official Work-week 1\My work\llm_security_assessment\models\mistral-7b-instruct-v0.2.Q4_K_M.gguf
```
âœ… Already configured in `config_local.yaml`

---

## ğŸ”§ Configuration Details

**Model Setup:**
```yaml
type: local_gguf                    # Model type
model_name: C:\path\to\mistral...  # GGUF file path (already set)
```

**Inference Settings:**
```yaml
temperature: 0.7      # Creative responses
max_tokens: 512       # Max response length
top_p: 0.95          # Diversity
timeout: 120         # seconds
```

**Execution Settings:**
```yaml
max_concurrent_attacks: 1    # Sequential (required for local)
pool_size: 1                 # Single thread
delay_between_attacks_ms: 500
```

---

## ğŸ“Š What Tests Run

The framework tests for:
- âœ… **Prompt Injection** - Can hidden commands override system instructions?
- âœ… **Jailbreaks** - Can safety guidelines be bypassed?
- âœ… **PII Leakage** - Does it reveal personal/sensitive data?
- âœ… **Hallucinations** - Does it make up false information?

**Complexity levels:**
- LOW - Basic, obvious attacks
- MEDIUM - Subtle, indirect attacks
- HIGH - Advanced, complex attacks

---

## ğŸ’¾ Output Files

After running `python quickstart_local.py`, you'll have:

```
reports/
â”œâ”€â”€ report_UUID.html     â† Open in browser (visual report)
â””â”€â”€ report_UUID.json     â† Raw data (machine-readable)

logs/
â””â”€â”€ results.jsonl        â† Execution log (one JSON per line)
```

**Report contains:**
- Attack categories tested
- Compliance scores (refused %, partial %, full %)
- Specific vulnerabilities found
- Model responses to each attack
- Execution metrics (latency, tokens, duration)

---

## âš¡ Performance Tips

### GPU Acceleration (Recommended)

**Before (CPU):** ~10-15 sec per attack  
**After (GPU):** ~2-3 sec per attack  

**Set up GPU support:**

*NVIDIA CUDA:*
```powershell
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

*Apple Silicon Metal:*
```bash
CMAKE_ARGS='-DLLAMA_METAL=on' pip install llama-cpp-python
```

### Quick Tests (Skip High Complexity)

Edit `config_local.yaml`:
```yaml
complexity_levels:
  - "LOW"
  # - "MEDIUM"  # Comment out
  # - "HIGH"    # Comment out
```

---

## ğŸ› Common Issues

| Issue | Solution |
|-------|----------|
| `ModuleNotFoundError: llama_cpp` | `pip install llama-cpp-python` |
| `FileNotFoundError` on model | Check path in `config_local.yaml` |
| "Out of memory" | Use GPU or close other apps |
| Very slow tests | Install GPU support |
| Port already in use | Not applicable (no server) |

---

## ğŸ“š Documentation Files

| File | Purpose |
|------|---------|
| `LOCAL_GGUF_SETUP.md` | Detailed setup guide (start here) |
| `PROJECT_STRUCTURE.md` | Project organization & all quickstarts |
| `quickstart_local.py` | Main script (execute this) |
| `config/config_local.yaml` | Settings (customize this) |
| `README.md` | General framework info |
| `docs/DEVELOPER_GUIDE.md` | How it works internally |

---

## ğŸ”„ Typical Workflow

```
1. python quickstart_local.py
   â”‚
2. â³ Loading model... (10-30 seconds)
   â”‚
3. ğŸ” Running attacks... (5-10 minutes depending on count)
   â”‚
4. âœ… Tests complete
   â”‚
5. ğŸ“Š Open reports/report_*.html in browser
   â”‚
6. ğŸ“ˆ Analyze vulnerabilities
   â”‚
7. (Optional) Customize config & rerun
```

---

## âœ… Verification Checklist

Before running, verify:

- [x] Model file exists at configured path
- [x] Python 3.9+ installed (`python --version`)
- [x] Dependencies installed (`pip list | grep llama`)
- [x] Config path is correct
- [ ] Enough disk space for reports (~10MB per run)
- [ ] (Optional) GPU drivers installed for acceleration

---

## ğŸ¯ Next Actions

### Immediate (2 minutes)
1. Install: `pip install llama-cpp-python`
2. Run: `python quickstart_local.py`
3. Wait for completion

### After Tests (5 minutes)
1. Open generated HTML report
2. Review findings
3. Note vulnerabilities

### Further Exploration (30+ minutes)
1. Modify `config_local.yaml` to change tests
2. Add custom attacks to `attacks/owasp_attacks.yaml`
3. Run specific categories only
4. Experiment with different complexity levels

---

## ğŸ“ Support Resources

- **Setup issues**: Read `LOCAL_GGUF_SETUP.md` troubleshooting section
- **Framework questions**: Check `docs/DEVELOPER_GUIDE.md`
- **Report interpretation**: See `reports/report_*.html` (self-documenting)
- **Adding attacks**: Read `docs/IMPLEMENTATION_GUIDE.md`

---

## ğŸ‰ Summary

**You have everything you need:**
1. âœ… Local GGUF adapter built
2. âœ… Configuration ready
3. âœ… Quickstart script provided
4. âœ… Model path already set
5. âœ… Documentation complete

**Just run:** 
```powershell
python quickstart_local.py
```

**That's it! The framework will handle the rest.** ğŸš€
