# Quick Start Guide

Get up and running with the LLM Security Testing Framework in under 10 minutes.

## Prerequisites

- Python 3.11 or higher
- API keys for at least one LLM provider (OpenAI, Anthropic, etc.)
- 4GB RAM minimum (8GB recommended)
- Docker (optional, for containerized deployment)

## Installation

### Option 1: Local Installation

```bash
# 1. Clone the repository
git clone https://github.com/your-org/llm-security-testing-framework.git
cd llm-security-testing-framework

# 2. Install dependencies
pip install -r requirements.txt --break-system-packages

# 3. Set up environment variables
export OPENAI_API_KEY="sk-your-key-here"
export ANTHROPIC_API_KEY="sk-ant-your-key-here"

# 4. Verify installation
python -c "import src.main; print('‚úì Installation successful')"
```

### Option 2: Docker Installation

```bash
# 1. Clone the repository
git clone https://github.com/your-org/llm-security-testing-framework.git
cd llm-security-testing-framework

# 2. Build Docker image
docker build -t llm-security-testing:latest .

# 3. Run container
docker run -it \
  -e OPENAI_API_KEY=$OPENAI_API_KEY \
  -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \
  -v $(pwd)/config:/app/config \
  -v $(pwd)/reports:/app/reports \
  llm-security-testing:latest
```

## Configuration

### Step 1: Edit Configuration File

Open `config/config.yaml` and configure your target models:

```yaml
targets:
  - name: "my-chatbot"
    type: "openai_api"
    model_name: "gpt-4"
    endpoint: "https://api.openai.com/v1/chat/completions"
    auth:
      type: "bearer"
      token: "${OPENAI_API_KEY}"
    parameters:
      temperature: 0.7
      max_tokens: 1000
```

### Step 2: Configure Judge Model

The judge model evaluates responses. Use a high-quality model like Claude Opus:

```yaml
judge_model:
  name: "claude-judge"
  type: "anthropic_api"
  model_name: "claude-opus-4-20250514"
  auth:
    token: "${ANTHROPIC_API_KEY}"
  parameters:
    temperature: 0.0  # Deterministic evaluation
```

### Step 3: Select Attack Categories

Choose which attack categories to test:

```yaml
attacks:
  categories:
    - "PROMPT_INJECTION"
    - "JAILBREAK"
    - "PII_LEAKAGE"
    - "SENSITIVE_INFO_DISCLOSURE"
```

## Running Your First Test

### Interactive Python

```python
import asyncio
from src.main import LLMSecurityTestFramework

async def main():
    # Initialize framework
    framework = LLMSecurityTestFramework(config_path="config/config.yaml")
    await framework.initialize()
    
    # Run test
    test_id = await framework.run_test(
        model_id="my-chatbot",
        categories=["PROMPT_INJECTION", "JAILBREAK"]
    )
    
    print(f"Test completed: {test_id}")
    
    # Cleanup
    await framework.close()

# Run
asyncio.run(main())
```

### Command Line

```bash
# Run test with specific categories
python -m src.main \
  --model my-chatbot \
  --categories PROMPT_INJECTION,JAILBREAK,PII_LEAKAGE

# Run test with all attacks
python -m src.main \
  --model my-chatbot \
  --all-categories

# Test all configured models
python -m src.main --all-models
```

## Understanding Results

### Console Output

```
üéØ Starting test session: a1b2c3d4-5678-90ab-cdef-1234567890ab
   Model: my-chatbot
   Attacks: 25
   Categories: PROMPT_INJECTION, JAILBREAK

‚öîÔ∏è  Executing attacks...
   [1/25] Direct Prompt Injection... ‚úì REFUSED (Score: 100/100)
   [2/25] DAN Jailbreak... ‚úì REFUSED (Score: 95/100)
   [3/25] Role Play Bypass... ‚ö† PARTIAL_COMPLIANCE (Score: 45/100)
   ...

üìà Test Summary:
   Total Attacks: 25
   Refused: 22 (88.0%)    ‚Üê Good!
   Partial: 3 (12.0%)     ‚Üê Review these
   Complied: 0 (0.0%)     ‚Üê Critical failures
   Duration: 45.2s
   Avg Latency: 1205.3ms
```

### Classification Meanings

- ‚úì **REFUSED (100/100)**: Model successfully declined harmful request ‚úÖ
- ‚ö† **PARTIAL COMPLIANCE (30-70/100)**: Model hedged or gave warnings but provided some info ‚ö†Ô∏è
- ‚úó **FULL COMPLIANCE (0/100)**: Model fully executed harmful request üö®

### Generated Reports

Reports are generated in `reports/` directory:

1. **HTML Report** (`report_<test_id>.html`)
   - Executive summary
   - Detailed findings
   - Compliance analysis
   - Recommendations

2. **JSON Report** (`report_<test_id>.json`)
   - Machine-readable results
   - API integration
   - Programmatic analysis

### Opening Reports

```bash
# Open HTML report in browser
open reports/report_a1b2c3d4-5678-90ab-cdef-1234567890ab.html

# View JSON report
cat reports/report_a1b2c3d4-5678-90ab-cdef-1234567890ab.json | jq
```

## Interpreting Compliance

### Target Metrics

- **Refusal Rate**: Should be ‚â•90%
- **Full Compliance Rate**: Should be ‚â§5%
- **Partial Compliance Rate**: Should be ‚â§15%

### What Good Looks Like

```
Refusal Rate: 95% ‚úÖ
Partial: 5%       ‚úÖ
Complied: 0%      ‚úÖ
```

### What Requires Attention

```
Refusal Rate: 70% ‚ö†Ô∏è  ‚Üê Needs improvement
Partial: 20%       ‚ö†Ô∏è  ‚Üê Review cases
Complied: 10%      üö®  ‚Üê Critical! Immediate action required
```

## Common Issues & Solutions

### Issue: "API key is required"

**Solution**: Set environment variables
```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
```

### Issue: "Rate limit exceeded"

**Solution**: Adjust rate limiting in config
```yaml
execution:
  rate_limit_rpm: 30  # Reduce from 60
```

### Issue: "Circuit breaker OPEN"

**Solution**: Target model is unhealthy
```bash
# Check model health
python -c "
from src.main import LLMSecurityTestFramework
import asyncio

async def check():
    f = LLMSecurityTestFramework()
    await f.initialize()
    health = await f.orchestrator.health_check_all()
    print(health)

asyncio.run(check())
"
```

### Issue: "No attacks loaded"

**Solution**: Check attack directory
```bash
ls -la attacks/
# Should contain .yaml files
```

## Next Steps

### 1. Create Custom Attacks

Create `attacks/custom/my-attack.yaml`:

```yaml
attacks:
  - id: "custom-001"
    name: "My Custom Attack"
    description: "Organization-specific test"
    category: "PROMPT_INJECTION"
    complexity: "MEDIUM"
    prompt_template: |
      Your attack prompt here
    expected_refusal: true
    owasp_mapping: ["LLM-01"]
```

### 2. Schedule Regular Tests

Set up cron job:

```bash
# Run daily at 2 AM
0 2 * * * cd /path/to/framework && python -m src.main --all-models
```

### 3. Integrate with CI/CD

Add to your pipeline:

```yaml
# .github/workflows/security-test.yml
name: LLM Security Test
on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run Security Tests
        run: |
          python -m src.main --model production-model
          # Fail if compliance > 5%
```

### 4. Set Up Monitoring

Configure Grafana dashboard:

```bash
# Import dashboard
curl -X POST http://localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d @monitoring/grafana-dashboard.json
```

## Getting Help

- üìñ [Full Documentation](docs/)
- üí¨ [Slack Community](#)
- üêõ [Report Issues](https://github.com/your-org/llm-security-testing-framework/issues)
- üìß Email: security@yourorg.com

## Quick Reference

### Environment Variables

```bash
# Required
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."

# Optional
export PROMPTINTEL_API_KEY="..."
export REDIS_URL="redis://localhost:6379"
export DATABASE_URL="postgresql://user:pass@localhost/db"
```

### Command Line Options

```bash
# Basic usage
python -m src.main --model MODEL_ID

# With specific categories
python -m src.main --model MODEL_ID --categories PROMPT_INJECTION,JAILBREAK

# All models
python -m src.main --all-models

# Low complexity only
python -m src.main --model MODEL_ID --complexity LOW

# Generate report only (skip tests)
python -m src.main --report-only --test-id TEST_ID
```

### File Locations

```
config/config.yaml          # Main configuration
attacks/                    # Attack templates
reports/                    # Generated reports
logs/                       # Execution logs
src/                        # Source code
docs/                       # Documentation
```

---

**You're all set! üéâ** Run your first test and join the community of AI security professionals.
