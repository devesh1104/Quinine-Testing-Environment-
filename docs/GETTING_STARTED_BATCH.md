# ğŸš€ Getting Started with Batch Testing

You now have three ways to test your LLM models!

---

## ğŸ¯ Quick Start (Pick One)

### âš¡ Option 1: Single Model Test (Fastest)
```bash
python src/main.py
```
âœ… Tests one model  
â±ï¸ 5-10 minutes  
ğŸ’° ~$0  

### ğŸ“Š Option 2: All Models Test (Comprehensive)
```bash
python src/main.py --mode=all
```
âœ… Tests all registered models  
â±ï¸ 20-30 minutes  
ğŸ’° ~$0.50-2  

### ğŸš€ Option 3: Batch Test Suite (Professional)
```bash
python src/main.py --mode=batch
```
âœ… Multiple test scenarios across all models  
â±ï¸ 60-120 minutes  
ğŸ’° $1-5  

---

## ğŸ“‹ What You Can Test

### Pick Your Models
```bash
# Gemini (free tier)
python src/main.py --config=config/config_gemini.yaml

# Ollama (free, local)
python src/main.py --config=config/config_ollama.yaml

# Promptintel (professional)
python src/main.py --config=config/config_promptintel.yaml
```

### Pick Your Categories
```bash
# Just prompt injection
python src/main.py --categories=PROMPT_INJECTION

# Multiple categories
python src/main.py --categories=PROMPT_INJECTION,JAILBREAK,PII_LEAKAGE
```

### Pick Your Complexity
```bash
# Only low-complexity attacks
python src/main.py --complexity=LOW

# Low and medium
python src/main.py --complexity=LOW,MEDIUM

# Everything
python src/main.py --complexity=LOW,MEDIUM,HIGH
```

---

## ğŸ“ Learning Path

### Step 1: First Test (5 minutes)
```bash
python src/main.py
```
- Tests default Gemini model
- Low complexity only
- Quick validation
- Generates report in `reports/`

### Step 2: All Models (15 minutes)
```bash
python src/main.py --mode=all --complexity=LOW
```
- Tests every model in your config
- Same settings for all
- Good for comparison

### Step 3: Full Batch (60 minutes)
```bash
python src/main.py --mode=batch
```
- Multiple test scenarios
- Different complexity levels
- Professional assessment

### Step 4: Custom Tests
```python
# See examples_batch_testing.py
python examples_batch_testing.py
```
- Choose predefined examples
- Learn the API
- Create custom tests

---

## ğŸ’¡ Smart Examples

### Example 1: Quick Daily Check
```bash
# Every morning - 5 minutes
python src/main.py --mode=all --complexity=LOW
```

### Example 2: Compare Providers
```bash
# Morning
python src/main.py --mode=all --config=config/config_gemini.yaml

# Afternoon  
python src/main.py --mode=all --config=config/config_ollama.yaml

# Evening
python src/main.py --mode=all --config=config/config_promptintel.yaml

# Check reports/ folder for comparison
```

### Example 3: Production Security Audit
```bash
python src/main.py --mode=batch
```
- Takes ~1 hour
- Tests all models multiple ways
- Generates comprehensive reports

### Example 4: Unlimited Local Testing
```bash
# No API costs, unlimited testing
python src/main.py --mode=batch --config=config/config_ollama.yaml
```

---

## ğŸ“Š Understanding Results

### Reports Generated
Each test creates:
- ğŸ“„ `report_[id].html` - Beautiful visual report
- ğŸ“‹ `report_[id].json` - Machine-readable data
- ğŸ“ˆ Graphs and metrics
- ğŸ”’ Compliance analysis

### Example Output
```
ğŸ¯ BATCH TEST SUITE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[1/2] Configuration: âš¡ Quick Validation - Low Complexity
   Models: gemini-flash, llama-local
   
   Testing: gemini-flash
   [1/3] Prompt Injection Test 1... âœ… REFUSED (Score: 100/100)
   [2/3] Prompt Injection Test 2... âœ… REFUSED (Score: 100/100)
   [3/3] Prompt Injection Test 3... âœ… REFUSED (Score: 100/100)
   
   Testing: llama-local
   [1/3] Prompt Injection Test 1... âš ï¸ PARTIAL (Score: 45/100)
   [2/3] Prompt Injection Test 2... âš ï¸ PARTIAL (Score: 38/100)
   [3/3] Prompt Injection Test 3... âŒ COMPLIED (Score: 0/100)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… BATCH TEST SUMMARY

Configuration Results:
  âœ… âš¡ Quick Validation: 6 tests

Reports: reports/report_[id].html
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ”Œ How to Use All Three Modes

### Mode 1: Single Model

**When to use:**
- Quick testing during development
- Debugging a specific model
- Testing API integration

**Command:**
```bash
python src/main.py --model=gemini-flash --complexity=LOW
```

**Output:**
```
1 HTML report
1 JSON report
5-10 minutes
```

### Mode 2: All Models

**When to use:**
- Comparing models
- Validating across all providers
- Consistent testing

**Command:**
```bash
python src/main.py --mode=all --categories=PROMPT_INJECTION
```

**Output:**
```
3+ HTML reports
3+ JSON reports
20-30 minutes
```

### Mode 3: Batch Suite

**When to use:**
- Production security audits
- Comprehensive assessment
- Professional reports

**Command:**
```bash
python src/main.py --mode=batch
```

**Output:**
```
6+ HTML reports
6+ JSON reports
Optional comparison report
60-120 minutes
```

---

## ğŸ› ï¸ Configuration Files

### config/config_gemini.yaml
```bash
# Google Gemini (free tier)
python src/main.py --config=config/config_gemini.yaml
```

### config/config_ollama.yaml
```bash
# Local Ollama models (unlimited, free)
python src/main.py --config=config/config_ollama.yaml
```

### config/config_promptintel.yaml
```bash
# Promptintel security prompts
python src/main.py --config=config/config_promptintel.yaml
```

### config/test_suites.yaml
```yaml
# Define your own test suites
test_suites:
  - name: "My Test"
    categories: ["PROMPT_INJECTION"]
    complexity_levels: ["LOW"]
```

---

## ğŸš¦ Decision Tree

```
What do you want to test?

â”œâ”€ Just checking one model quickly?
â”‚  â””â”€â–º python src/main.py
â”‚
â”œâ”€ Want to compare all available models?
â”‚  â””â”€â–º python src/main.py --mode=all
â”‚
â”œâ”€ Need comprehensive audit?
â”‚  â””â”€â–º python src/main.py --mode=batch
â”‚
â”œâ”€ Want unlimited free testing?
â”‚  â””â”€â–º python src/main.py --config=config/config_ollama.yaml
â”‚
â”œâ”€ Testing specific categories?
â”‚  â””â”€â–º python src/main.py --categories=PROMPT_INJECTION,JAILBREAK
â”‚
â””â”€ Creating custom test script?
   â””â”€â–º python examples_batch_testing.py
```

---

## ğŸ“š Documentation Map

| Document | Purpose | Read Time |
|----------|---------|-----------|
| [QUICK_REF.md](QUICK_REF.md) | Command cheat sheet | 5 min |
| [BATCH_TESTING.md](docs/BATCH_TESTING.md) | Complete guide | 15 min |
| [BATCH_TESTING_SUMMARY.md](BATCH_TESTING_SUMMARY.md) | Feature overview | 10 min |
| [SETUP.md](docs/SETUP.md) | Initial setup | 20 min |
| [examples_batch_testing.py](examples_batch_testing.py) | Runnable code | 10 min |

---

## ğŸ¯ Next Steps

### Step 1: Run First Test
```bash
python src/main.py
```

### Step 2: Check Reports
```bash
# Open in browser
start reports/report_*.html
```

### Step 3: Try All Models
```bash
python src/main.py --mode=all
```

### Step 4: Run Batch Suite
```bash
python src/main.py --mode=batch
```

### Step 5: Create Custom Tests
Edit `config/test_suites.yaml` or `examples_batch_testing.py`

---

## ğŸ’° Cost Estimates

### Option 1: Free
```bash
python src/main.py --config=config/config_ollama.yaml
# Local testing, zero cost, unlimited
```

### Option 2: Very Cheap
```bash
python src/main.py --mode=all --config=config/config_gemini.yaml
# Google Gemini free tier, 30 tests/day free
```

### Option 3: Budget-Friendly
```bash
python src/main.py --mode=batch
# Mix of free (Gemini) + local (Ollama) + Promptintel trial
# Estimated: $0-5 per batch
```

### Option 4: Professional
```bash
# Use Promptintel + Gemini paid tier
# Full compliance reporting
# Estimated: $5-20 per comprehensive audit
```

---

## âš™ï¸ Performance Tips

### ğŸš€ Speed It Up
```bash
# Use only LOW complexity
python src/main.py --complexity=LOW

# Test one category
python src/main.py --categories=PROMPT_INJECTION

# Use Ollama (no API latency)
python src/main.py --config=config/config_ollama.yaml
```

### ğŸ’ª Get More Coverage
```bash
# Use ALL complexity levels
python src/main.py --complexity=LOW,MEDIUM,HIGH

# Test ALL categories
python src/main.py --categories=PROMPT_INJECTION,JAILBREAK,PII_LEAKAGE

# Test ALL models
python src/main.py --mode=all
```

### ğŸ’° Keep Costs Down
```bash
# Use free Gemini tier
python src/main.py --config=config/config_gemini.yaml --complexity=LOW

# Or use local Ollama (completely free)
python src/main.py --config=config/config_ollama.yaml --mode=batch
```

---

## ğŸ”’ API Key Setup (5 minutes)

### Gemini (Recommended - Free)
```bash
# 1. Go to: https://ai.google.dev/pricing
# 2. Get free API key
# 3. Set it:
export GEMINI_API_KEY="your_key_here"

# 4. Test
python src/main.py
```

### Ollama (Free - Local)
```bash
# 1. Download: https://ollama.ai
# 2. Run: ollama serve
# 3. Test
python src/main.py --config=config/config_ollama.yaml
```

### Promptintel (Professional)
```bash
# 1. Sign up: https://promptintel.novahunting.ai
# 2. Get API key
# 3. Set it:
export PROMPTINTEL_API_KEY="your_key"

# 4. Test
python src/main.py --config=config/config_promptintel.yaml
```

---

## ğŸš¨ Common Issues

### "API key not found"
```bash
# Make sure environment variable is set
echo $GEMINI_API_KEY

# If empty, set it
export GEMINI_API_KEY="your_key"
```

### "Connection refused" (Ollama)
```bash
# Start Ollama
ollama serve

# In another terminal, run tests
python src/main.py --config=config/config_ollama.yaml
```

### "Tests too slow"
```bash
# Use Ollama (local, no API latency)
python src/main.py --config=config/config_ollama.yaml

# Or test LOW complexity only
python src/main.py --complexity=LOW
```

### "Rate limit exceeded"
```yaml
# Edit your config:
execution:
  rate_limit_rpm: 5  # Reduce from 15
  delay_between_attacks_ms: 5000  # Increase from 2000
```

---

## ğŸ“ Support

- ğŸ“– **Docs:** See files in `docs/` folder
- ğŸ” **Examples:** Run `examples_batch_testing.py`
- ğŸ’¡ **Quick Ref:** See `QUICK_REF.md`
- ğŸ“Š **Issues:** Check troubleshooting section above

---

## âœ… You're All Set!

You now have:
- âœ… Three testing modes (single, all, batch)
- âœ… Support for multiple providers (Gemini, Ollama, Promptintel)
- âœ… CLI interface with full argument support
- âœ… Comprehensive documentation
- âœ… Runnable examples
- âœ… Beautiful reports

### ğŸ‰ Ready to Test!

```bash
# Try it now:
python src/main.py

# Or go advanced:
python src/main.py --mode=batch
```

---

**Happy testing! ğŸš€**
